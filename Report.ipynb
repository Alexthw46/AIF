{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf21ca49",
   "metadata": {},
   "source": [
    "# üçè A*pple Quest  \n",
    "**Artificial Intelligence Course Project**  \n",
    "**Authors:** Alessandro Querci, Andrea Lepori  \n",
    "**Academic Year:** 2024/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea92d9b",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "This notebook constitutes the report for the final project of the AIF course, project's name is \"A*pple Quest\" and our group is composed by Alessandro Querci and Andrea Lepori. The goal of this project is to explore classical Artificial Intelligence **search and planning algorithms**, among the ones seen during the course, within the MiniHack environment, a reinforcement learning platform built on top of NetHack. Our work started by trying to identify a task suitable to apply the methodologies and algorithms seen in class (GOFAI, no learning). In particular, we decided to focus on the use of state space search and planning algorithms. The high-level idea is to design some custom rooms/environments containing apples (with associated reward) and a custom reward manager. The reward manager assigns a positive reward for each collected apple and a (typically larger) reward when the agent reaches the downstairs by successfully completing the task. Additionally, to incentivize the agent to be as efficient as possible in terms of the number of steps, a (small) constant penalty is assigned for each step the agent moves through. Then, to evaluate the different search algorithms, they are used to generate a path to complete the task (reaching the downstairs) while maximizing the reward. We decided to test the algotithms both in an *offline setting* with *full observability* and in an **online setting** with **partial observability**. The environment is determinstic, discrete, static, episodic and single agent. As mentioned above, if we are in the offline setting the observability is full, in the online setting the observability is partial. To test our algorithms we defined different rooms/environments/mazes with different complexity. We started with a simple rectangular room, named \"simple room\", with some random apples and no obstacles, to start testing and debugging the algorithms, then we add obstacles like lava in the so-called \"lava room\" that allow more difficult tasks while preserving the full observability. Then we introduce walls that reduce the visibility of the agent as you can see in the \"simple_maze.des\" and in the more difficult \"complex_maze.des\". To compare and evaluate each algorithm the more natural metric is the **total reward** collected by the agent accomplishing the task and computing automatically by the Reward Manager, however we decide to evaluate the algorithms across other metrics: **success rate** (task-completion/reaching the downstairs), **planning time** (time to elaborate a path/plan), **path length** (number of steps in the path), **apples collected** (numbers of collected apples during the path). Once the algorithms were written and the environments defined, a model selection was carried out using grid search to optimize some of the hyperparameters. For all algorithms, given the same environment, several runs/simulations were executed with randomized apple positions, starting points, and downstairs, and the average of the metrics of interest was calculated in order to compare the algorithms across various dimensions and evaluate the trade-offs in using one over another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b2759",
   "metadata": {},
   "source": [
    "# Related works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f622b7",
   "metadata": {},
   "source": [
    "# Methodologies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866722dd",
   "metadata": {},
   "source": [
    "# Assessment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ee3c3",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05d0e3",
   "metadata": {},
   "source": [
    "## Results \n",
    "### Offline benchmark on \"lava_env\", best result in terms of Reward for each algorithm\n",
    "| Algorithm   | Avg Reward | Avg Path Length | Avg Apples | Avg Success Rate | Avg Planning Time |\n",
    "|-------------|------------|-----------------|------------|------------------|-------------------|\n",
    "|A* Star      |  1.065\t   |      34.7       |    5.0     |        1.0       |      0.006        |\n",
    "|MCTS         |  0.715     |      27.4       |    3.1     |        1.0       |      2.339        |\n",
    "|Greedy BFS   |  0.85      |      35.0       |    5.0     |        1.0       |      0.054        |\n",
    "|Pot. Fields  |  1.030     |      34.1       |    4.6     |        1.0       |                   |\n",
    "|Beam Search  |            |                 |            |                  |                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9efb7",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df2818",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    "- `Benchmark_Offline.ipynb` ‚Äî Notebook for offline benchmark experiments\n",
    "- `Benchmark_Online.ipynb` ‚Äî Notebook for online benchmark experiments  \n",
    "- `MCTS.py` ‚Äî Implementation of Monte Carlo Tree Search   \n",
    "- `algorithms.py` ‚Äî Search & Planning algorithms\n",
    "- `algorithms_online.py` ‚Äî Online search algorithms \n",
    "- `complex_maze.des` ‚Äî Complex maze environment description file  \n",
    "- `simple_maze.des` ‚Äî Simple maze environment \n",
    "- `simulator.py` ‚Äî Implementation of the various simulators\n",
    "- `utils.py` ‚Äî Utility functions \n",
    "- `Report.ipynb` ‚Äî Project Report (only Text/Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c6386",
   "metadata": {},
   "source": [
    "## team contributions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ccb32",
   "metadata": {},
   "source": [
    "## relationship with the course"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
