{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf21ca49",
   "metadata": {},
   "source": [
    "# 🍏 A*pple Quest  \n",
    "**Artificial Intelligence Course Project**  \n",
    "**Authors:** Alessandro Querci, Andrea Lepori  \n",
    "**Academic Year:** 2024/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea92d9b",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "This notebook constitutes the report for the final project of the AIF course, project's name is \"A*pple Quest\" and our group is composed by Alessandro Querci and Andrea Lepori. The goal of this project is to explore classical Artificial Intelligence **search and planning algorithms**, among the ones seen during the course, within the MiniHack environment, a reinforcement learning platform built on top of NetHack. Our work started by trying to identify a task suitable to apply the methodologies and algorithms seen in class (GOFAI, no learning). In particular, we decided to focus on the use of state space search and planning algorithms. The high-level idea is to design some custom rooms/environments containing apples (with associated reward) and a custom reward manager. The reward manager assigns a positive reward for each collected apple and a (typically larger) reward when the agent reaches the downstairs by successfully completing the task. Additionally, to incentivize the agent to be as efficient as possible in terms of the number of steps, a (small) constant penalty is assigned for each step the agent moves through. Then, to evaluate the different search algorithms, they are used to generate a path to complete the task (reaching the downstairs) while maximizing the reward. We decided to test the algotithms both in an *offline setting* with *full observability* and in an **online setting** with **partial observability**. The environment is determinstic, discrete, static, episodic and single agent. As mentioned above, if we are in the offline setting the observability is full, in the online setting the observability is partial. To test our algorithms we defined different rooms/environments/mazes with different complexity. We started with a simple rectangular room, named \"simple room\", with some random apples and no obstacles, to start testing and debugging the algorithms, then we add obstacles like lava in the so-called \"lava room\" that allow more difficult tasks while preserving the full observability. Then we introduce walls that reduce the visibility of the agent as you can see in the \"simple_maze.des\" and in the more difficult \"complex_maze.des\". To compare and evaluate each algorithm the more natural metric is the **total reward** collected by the agent accomplishing the task and computing automatically by the Reward Manager, however we decide to evaluate the algorithms across other metrics: **success rate** (task-completion/reaching the downstairs), **planning time** (time to elaborate a path/plan), **path length** (number of steps in the path), **apples collected** (numbers of collected apples during the path). Once the algorithms were written and the environments defined, a model selection was carried out using grid search to optimize some of the hyperparameters. For all algorithms, given the same environment, several runs/simulations were executed with randomized apple positions, starting points, and downstairs, and the average of the metrics of interest was calculated in order to compare the algorithms across various dimensions and evaluate the trade-offs in using one over another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b2759",
   "metadata": {},
   "source": [
    "# Related works\n",
    "To realize our project, we relied heavily on MiniHack. \n",
    "> \"MiniHack is a sandbox framework for easily designing rich and diverse environments for Reinforcement Learning (RL). Based on the game of NetHack, MiniHack uses the NetHack Learning Environment (NLE) to communicate with the game and to provide a convenient interface for customly created RL training and test environments of varying complexity.\" \n",
    "\n",
    "> — https://github.com/samvelyan/minihack\n",
    "\n",
    "In particular, we studied and heavily used the APIs provided by Minihack to customize our environment, customize the reward manager, manage the simulation, etc...\n",
    "\n",
    "We took also ispiration, as a starting point, from the hands-on sessions of the course and from some projects of previous students like \"Get The Gold!\" by Federico Cioni, Claudia Gentili and Giovanni Guerrazzi (https://github.com/fefex302/AIF_minihack_project/) that have applied search algorithms but to a different task and to a different environment. In particular they defined a customized Minihack environment, the gold room, essentially a square or rectangular room filled with some pieces of gold and some leprechauns. The aim of their agent is to reach the exit stairs, maximizing the gain with the leprechauns that act as opponents because their main goal is to take all the gold they manage. In particular they studied scenarios with full observability and non deterministic. In our work, apart from focusing on a different task and environment, we extend the application of the search algorithms to environment with partial observability but we restrict to completely deterministic environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f622b7",
   "metadata": {},
   "source": [
    "# Methodologies "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03c598",
   "metadata": {},
   "source": [
    "# Assessment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05d0e3",
   "metadata": {},
   "source": [
    "## Results \n",
    "### Offline benchmark on \"lava env\", best averaged result in terms of Reward for each algorithm\n",
    "| Algorithm   | Avg Reward | Avg Path Length | Avg Apples | Avg Success Rate | Avg Planning Time |\n",
    "|-------------|------------|-----------------|------------|------------------|-------------------|\n",
    "|A* Star      |  1.065\t   |      34.7       |    5.0     |        1.0       |      0.00         |\n",
    "|MCTS         |  0.715     |      27.4       |    3.1     |        1.0       |      2.33         |\n",
    "|Greedy BFS   |  0.85      |      35.0       |    5.0     |        1.0       |      0.05         |\n",
    "|Pot. Fields  |  0.745     |      34.1       |    4.7     |        1.0       |      0.04         |\n",
    "|Beam Search  |  0.055\t   |      29.3       |    2.9     |        1.0\t     |      0.09         |\n",
    "\n",
    "### Online benchmark on \"simple maze\", best averaged result in terms of Reward for each algorithmn \n",
    "| Algorithm | Avg Reward | Avg Path Length | Avg Apples | Avg Success Rate | Avg Planning Time |\n",
    "|-----------|------------|-----------------|------------|------------------|-------------------|\n",
    "|A* Star    | 4.822      | 84.8            | 4.4        | 1.0              | 0.08              |\n",
    "|MCTS       | 4.954      | 77.1            | 4.5        | 1.0              | 0.87              |\n",
    "|Greedy BFS | 5.351      | 81.9            | 4.9        | 1.0              | 0.07              |\n",
    "|Pot. Fields| 4.776      | 131.1           | 4.8        | 1.0              | 0.082             |\n",
    "|Beam Search| 4.390      | 73.8            | 1.0        | 3.9              | 0.18              |\n",
    "\n",
    "### Online benchmark on \"complex maze\", best averaged result in terms of Reward for each algorithmn \n",
    "| Algorithm   | Avg Reward | Avg Path Length | Avg Apples | Avg Success Rate | Avg Planning Time |\n",
    "|-------------|------------|-----------------|------------|------------------|-------------------|\n",
    "| A* Star     | 3.684      | 139.2           | 3.8        | 1.0              | 0.23              |\n",
    "| MCTS        | 2.729      | 219.1           | 3.7        | 1.0              | 5.18              |\n",
    "| Greedy BFS  | 3.911      | 139.7           | 4.0        | 1.0              | 0.35              |\n",
    "| Pot. Fields | 3.345      | 147.5           | 3.6        | 1.0              | 0.16              |\n",
    "| Beam Search | 2.636      | 155.5           | 1.0        | 2.9              | 1.269             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ee3c3",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9efb7",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29df2818",
   "metadata": {},
   "source": [
    "## Project Structure\n",
    "\n",
    "- `Benchmark_Offline.ipynb` — Notebook for offline benchmark experiments\n",
    "- `Benchmark_Online.ipynb` — Notebook for online benchmark experiments  \n",
    "- `MCTS.py` — Implementation of Monte Carlo Tree Search   \n",
    "- `algorithms.py` — Search & Planning algorithms\n",
    "- `algorithms_online.py` — Online search algorithms \n",
    "- `complex_maze.des` — Complex maze environment description file  \n",
    "- `simple_maze.des` — Simple maze environment \n",
    "- `simulator.py` — Implementation of the various simulators\n",
    "- `utils.py` — Utility functions \n",
    "- `Report.ipynb` — Project Report (only Text/Markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1c6386",
   "metadata": {},
   "source": [
    "## Team contributions\n",
    "The project work was carried out by the two members of the group, in particular:\n",
    "- **Alessandro Querci**: implemented the logic of the simulators, a_star_apple, beam search, online algorithms, frontier search and benchmarking \n",
    "- **Andrea Lepori**: implemented a_star_collect_apples, MCTS, potential fields, Greedy Best First, wrote the report.\n",
    "The work on the project involved some meetings and pair programming sessions as well as individual work using github as a collaboration platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ccb32",
   "metadata": {},
   "source": [
    "## Relationship with the course\n",
    "The work of this project has strong relations with the topics of the AIF course, in particular with the section on state space search algorithms. Particular attention was paid to stick to the typology of classical AI algorithms studied in the course (no machine learning). However, we tried to experiment with algorithms not seen during the course such as Potential fields or lightly touched upon such as A* online or with heuristics specifically designed for our task such as bfs_cache. Furthermore, our work has a strong relationship with the practical lessons held during the hands-on sessions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
